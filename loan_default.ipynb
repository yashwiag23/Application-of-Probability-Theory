{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1192d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\yashwi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\yashwi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.12.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\yashwi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from xgboost) (1.26.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\yashwi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from xgboost) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\yashwi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from imbalanced-learn) (1.3.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\yashwi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from imbalanced-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\yashwi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from imbalanced-learn) (3.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a94ee643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ada0a422",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    r\"C:\\Users\\Yashwi\\OneDrive\\Desktop\\IITK\\Summer Project 2024\\Application of Probability Theory\\Loan Default\\accepted_2007_to_2018Q4.csv\\accepted_2007_to_2018Q4.csv\",\n",
    "    usecols=[\n",
    "        \"loan_status\", \"annual_inc\", \"fico_range_low\", \"dti\", \"loan_amnt\", \"purpose\",\n",
    "        \"int_rate\", \"emp_length\", \"revol_util\", \"delinq_2yrs\", \"home_ownership\", \"term\"\n",
    "    ],\n",
    "    nrows=2000000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09b4f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary classification\n",
    "df = df[df[\"loan_status\"].isin([\"Charged Off\", \"Fully Paid\"])]\n",
    "df[\"default\"] = df[\"loan_status\"].apply(lambda x: 1 if x == \"Charged Off\" else 0)\n",
    "\n",
    "# emp_length to numeric\n",
    "def parse_emp_length(x):\n",
    "    if pd.isnull(x):\n",
    "        return np.nan\n",
    "    if '<' in x:\n",
    "        return 0\n",
    "    if '10+' in x:\n",
    "        return 10\n",
    "    return int(x.split()[0])\n",
    "df[\"emp_length\"] = df[\"emp_length\"].apply(parse_emp_length)\n",
    "\n",
    "# drop missing rows\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# one-hot encode categorical variables\n",
    "df = pd.get_dummies(df, columns=[\"purpose\", \"home_ownership\", \"term\"], drop_first=True)\n",
    "\n",
    "# features and label\n",
    "X = df.drop(columns=[\"loan_status\", \"default\"])\n",
    "y = df[\"default\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52d4f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5df22bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd3a2295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale Pos Weight: 4.20\n"
     ]
    }
   ],
   "source": [
    "neg, pos = np.bincount(y_train)\n",
    "scale_pos_weight = neg / pos\n",
    "print(f\"Scale Pos Weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    objective=\"binary:logistic\",        # binary classification\n",
    "    eval_metric=\"logloss\",              # avoid deprecation warning\n",
    "    use_label_encoder=False,            # suppress label encoder warning\n",
    "    n_estimators=200,                   # number of trees\n",
    "    max_depth=5,                        # control overfitting\n",
    "    learning_rate=0.1,                  # step size (lower = more conservative)\n",
    "    subsample=0.8,                      # row sampling to reduce overfitting\n",
    "    colsample_bytree=0.8,               # feature sampling per tree\n",
    "    scale_pos_weight=scale_pos_weight,  # handle class imbalance\n",
    "    random_state=42                     # reproducibility\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_bal, y_train_bal)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "y_proba = xgb_model.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8451479e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Test Set Defaults: 155623\n",
      "Expected Defaults (mean): 44515.50\n",
      "Chebyshev's Upper Bound (95% confidence): 44894.78\n"
     ]
    }
   ],
   "source": [
    "# empirical stats\n",
    "n = len(y_test)\n",
    "actual_defaults = y_pred.sum()\n",
    "empirical_rate = y_train.mean()\n",
    "\n",
    "# theoretical mean and standard deviation\n",
    "expected_defaults = n * empirical_rate\n",
    "std_dev = np.sqrt(n * empirical_rate * (1 - empirical_rate))\n",
    "\n",
    "k = 2  # 95% confidence\n",
    "chebyshev_upper_bound = expected_defaults + k * std_dev\n",
    "\n",
    "print(\"Actual Test Set Defaults:\", actual_defaults)\n",
    "print(f\"Expected Defaults (mean): {expected_defaults:.2f}\")\n",
    "print(f\"Chebyshev's Upper Bound (95% confidence): {chebyshev_upper_bound:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f41e79bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Predicted Default Probability: 0.4351\n"
     ]
    }
   ],
   "source": [
    "# predict probability for sample borrower\n",
    "new_borrower = pd.DataFrame([{\n",
    "    \"annual_inc\": 2000000,\n",
    "    \"fico_range_low\": 700,\n",
    "    \"dti\": 12.5,\n",
    "    \"loan_amnt\": 8000,\n",
    "    \"int_rate\": 12.0,\n",
    "    \"emp_length\": 5,\n",
    "    \"revol_util\": 30.0,\n",
    "    \"delinq_2yrs\": 0,\n",
    "    # Set all one-hot encoded cols to 0\n",
    "    **{col: 0 for col in X.columns if col.startswith(\"purpose_\") or col.startswith(\"home_\") or col.startswith(\"term_\")}\n",
    "}], columns=X.columns)\n",
    "\n",
    "new_borrower_scaled = scaler.transform(new_borrower)\n",
    "prob_default = xgb_model.predict_proba(new_borrower_scaled)[0, 1]\n",
    "print(\"🔍 Predicted Default Probability:\", round(prob_default, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "32f5a7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4678\n",
      "AUC Score: 0.7005\n",
      "Recall for Defaulters (Class 1): 0.8629\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Fully Paid       0.92      0.37      0.53    187187\n",
      "     Default       0.25      0.86      0.38     44516\n",
      "\n",
      "    accuracy                           0.47    231703\n",
      "   macro avg       0.58      0.62      0.46    231703\n",
      "weighted avg       0.79      0.47      0.50    231703\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 69978 117209]\n",
      " [  6102  38414]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix, recall_score\n",
    "\n",
    "# accuracy (not the best for imbalanced data)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# AUC Score\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# classification report\n",
    "report = classification_report(y_test, y_pred, target_names=[\"Fully Paid\", \"Default\"])\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# extract recall for defaulters (class 1)\n",
    "recall_default = recall_score(y_test, y_pred)\n",
    "\n",
    "# Print everything\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"AUC Score: {auc:.4f}\")\n",
    "print(f\"Recall for Defaulters (Class 1): {recall_default:.4f}\")\n",
    "print(\"Classification Report:\\n\", report)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb4d7a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.809\n",
      "F1-Score: 0.040\n",
      "\n",
      "AUC score: 0.5085868626254635\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.89    187187\n",
      "           1       0.60      0.02      0.04     44516\n",
      "\n",
      "    accuracy                           0.81    231703\n",
      "   macro avg       0.70      0.51      0.47    231703\n",
      "weighted avg       0.77      0.81      0.73    231703\n",
      "\n",
      "Top 10 important features:\n",
      "int_rate: 0.4396\n",
      "term_ 60 months: 0.2174\n",
      "fico_range_low: 0.0891\n",
      "dti: 0.0721\n",
      "loan_amnt: 0.0503\n",
      "annual_inc: 0.0318\n",
      "home_ownership_MORTGAGE: 0.0260\n",
      "home_ownership_RENT: 0.0232\n",
      "revol_util: 0.0195\n",
      "emp_length: 0.0063\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# random forest\n",
    "rf_model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc_score = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"F1-Score: {f1:.3f}\")\n",
    "print(\"\\nAUC score:\", auc_score)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# important features\n",
    "importances = rf_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# top 10 features\n",
    "top_indices = np.argsort(importances)[-10:][::-1]\n",
    "print(\"Top 10 important features:\")\n",
    "for i in top_indices:\n",
    "    print(f\"{feature_names[i]}: {importances[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94b1d474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.809\n",
      "Average prediction latency: 0.000 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.029s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import unittest\n",
    "\n",
    "# logistic regression\n",
    "lr_model = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# prediction latency on test set\n",
    "start = time.time()\n",
    "y_pred = lr_model.predict(X_test)\n",
    "end = time.time()\n",
    "latency_per_sample = (end - start) / len(X_test) * 1000  # in ms\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Average prediction latency: {latency_per_sample:.3f} ms\")\n",
    "\n",
    "class TestModel(unittest.TestCase):\n",
    "    def test_predict_shape(self):\n",
    "        preds = lr_model.predict(X_test)\n",
    "        self.assertEqual(preds.shape[0], X_test.shape[0])\n",
    "\n",
    "    def test_predict_values(self):\n",
    "        preds = lr_model.predict(X_test)\n",
    "        self.assertTrue(set(preds).issubset({0,1}))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main(argv=[''], exit=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
